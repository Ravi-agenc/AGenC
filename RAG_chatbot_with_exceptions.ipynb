{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIth1fTF7e18dtB+7Cp4T0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ravi-agenc/AGenC/blob/main/RAG_chatbot_with_exceptions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDipZFiQdKt3"
      },
      "outputs": [],
      "source": [
        "\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "import os\n",
        "from langchain.chains.conversation.memory import ConversationBufferMemory, ConversationSummaryBufferMemory\n",
        "from langchain_community.llms import OpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.memory import ChatMessageHistory\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import TextLoader, DirectoryLoader\n",
        "import uvicorn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Environment setup\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "\n",
        "# Initialize FastAPI app\n",
        "app = FastAPI()\n"
      ],
      "metadata": {
        "id": "nWkJWL2zfBkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#! Already in the prompt_model.py file\n",
        "# Request model for user input\n",
        "class PromptInput(BaseModel):\n",
        "    prompt: str"
      ],
      "metadata": {
        "id": "dJH5ehxtfBiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the language model and memory\n",
        "llm = ChatOpenAI(model_name='gpt-3.5-turbo-0125', temperature=0, max_tokens=256)\n",
        "memory_1 = ConversationBufferMemory()\n",
        "conversation = ConversationChain(llm=llm, verbose=True, memory=memory_1)"
      ],
      "metadata": {
        "id": "T5LvnOFufBgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data - change as needed\n",
        "os.system('wget -q https://www.dropbox.com/s/vs6ocyvpzzncvwh/new_articles.zip')\n",
        "os.system('unzip -q new_articles.zip -d new_articles')"
      ],
      "metadata": {
        "id": "4KUi2gDgfBeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize document loader and process text files\n",
        "loader = DirectoryLoader('./new_articles/', glob=\"./*.txt\", loader_cls=TextLoader)\n",
        "documents = loader.load()\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "texts = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "SRbVHj-3fBci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and persist vector database\n",
        "persist_directory = 'db'\n",
        "embedding = OpenAIEmbeddings()\n",
        "vectordb = Chroma.from_documents(documents=texts, embedding=embedding, persist_directory=persist_directory)\n",
        "vectordb.persist()\n",
        "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)"
      ],
      "metadata": {
        "id": "xyNUu9CMfBae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a retriever and QA chain\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 2})\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n"
      ],
      "metadata": {
        "id": "6tPgJV74fBZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 5\n",
        "conv_counter = 0\n"
      ],
      "metadata": {
        "id": "YayVHjBrfBWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def switch_memory_if_needed(conversation_chain, input_text, conv_counter):\n",
        "    output = conversation_chain.predict(input=input_text)\n",
        "    conversation_length = len(conversation_chain.memory.buffer)\n",
        "\n",
        "    if conv_counter == threshold + 1:\n",
        "        memory_2 = ConversationSummaryBufferMemory(llm=llm, max_token_limit=150, chat_memory=ChatMessageHistory(messages=memory_1.chat_memory.messages))\n",
        "        conversation_chain.memory = memory_2\n",
        "        print(\"Switched to ConversationSummaryBufferMemory\")\n",
        "\n",
        "    return output\n",
        "\n"
      ],
      "metadata": {
        "id": "rB94Qg6gfBUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_llm_response(llm_response):\n",
        "    if llm_response['result']:\n",
        "        return llm_response['result']\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "ORGtrnDPfUaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_llm(prompt_input: PromptInput):\n",
        "    global conv_counter\n",
        "    try:\n",
        "        user_input = prompt_input.prompt\n",
        "        conv_counter += 1\n",
        "\n",
        "        # Use RAG to retrieve relevant documents and get the response\n",
        "        llm_response = qa_chain(user_input)\n",
        "        rag_response = process_llm_response(llm_response)\n",
        "\n",
        "        # Generate a general response based on conversation context\n",
        "        general_response = switch_memory_if_needed(conversation, user_input, conv_counter)\n",
        "\n",
        "        if rag_response:\n",
        "            return {\"response\": rag_response, \"source\": \"RAG\"}\n",
        "        else:\n",
        "            return {\"response\": general_response, \"source\": \"General\"}\n",
        "\n",
        "    except ValueError as ve:\n",
        "        raise HTTPException(status_code=400, detail=str(ve))\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "    except NotFound as nf:\n",
        "        raise HTTPException(status_code=404, detail=str(nf)\n",
        "\n"
      ],
      "metadata": {
        "id": "kNFlI5-ZfUWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XLrgahpGfUSH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}